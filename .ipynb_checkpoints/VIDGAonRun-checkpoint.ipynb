{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import pdb\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "import pyodbc \n",
    "from deep_emotion_recognition import DeepEmotionRecognizer\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import imutils\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 809 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 148 testing audio files for category:angry\n",
      "[TESS&RAVDESS] There are 813 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 586 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 94 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 514 training audio files for category:ps\n",
      "[TESS&RAVDESS] There are 78 testing audio files for category:ps\n",
      "[TESS&RAVDESS] There are 806 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 148 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 339\n",
      "[EMO-DB] Training samples: 271\n",
      "[EMO-DB] Testing samples: 67\n",
      "[+] Writed EMO-DB CSV File\n",
      "[Custom Dataset] There are 49 training audio files for category:neutral\n",
      "[Custom Dataset] There are 33 testing audio files for category:neutral\n",
      "[Custom Dataset] There are 33 training audio files for category:ps\n",
      "[Custom Dataset] There are 33 testing audio files for category:ps\n",
      "[Custom Dataset] There are 48 training audio files for category:happy\n",
      "[Custom Dataset] There are 23 testing audio files for category:happy\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.5230769230769231\n"
     ]
    }
   ],
   "source": [
    "#Loading visual emotion recognition model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Deep_Emotion()\n",
    "net.load_state_dict(torch.load('DeepEmotion_trained_by_VIDGA.pt'))\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "#defining sound recording function\n",
    "#Loading audio emotion recognition model\n",
    "\n",
    "def recordSound():\n",
    "    print(\".\")\n",
    "    samplerate = 44100  # Hertz\n",
    "    duration = 1  # seconds\n",
    "    filename = 'output.wav'\n",
    "\n",
    "    mydata = sd.rec(int(samplerate * duration), samplerate=samplerate,\n",
    "                    channels=1, blocking=True)\n",
    "    sf.write(filename, mydata, samplerate)\n",
    "\n",
    "# initialize instance\n",
    "# inherited from emotion_recognition.EmotionRecognizer\n",
    "# default parameters (LSTM: 128x2, Dense:128x2)\n",
    "deeprec = DeepEmotionRecognizer(emotions=['angry', 'sad', 'neutral', 'ps', 'happy'], n_rnn_layers=2, n_dense_layers=2, rnn_units=128, dense_units=128)\n",
    "# train the model\n",
    "deeprec.train()\n",
    "# get the accuracy\n",
    "print(deeprec.test_score())\n",
    "# predict angry audio sample\n",
    "\n",
    "\n",
    "##################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[293 265 194 194]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: neutral\n",
      "[[324 252 201 201]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: neutral\n",
      "[[285 250 206 206]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: neutral\n",
      "[[271 219 185 185]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[270 217 190 190]\n",
      " [516 186  26  26]]\n",
      "Alındı.\n",
      "else\n",
      "Alındı.\n",
      "Face not detected\n",
      "3\n",
      ".\n",
      "Prediction: neutral\n",
      "[[294 222 203 203]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: sad\n",
      "[[292 250 203 203]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[283 245 207 207]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: neutral\n",
      "[[293 248 204 204]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[288 246 205 205]]\n",
      "Alındı.\n",
      "else\n",
      "0\n",
      ".\n",
      "Prediction: neutral\n",
      "[[199 240 198 198]]\n",
      "Alındı.\n",
      "Face not detected\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[273 274 195 195]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: neutral\n",
      "[[544 186  26  26]\n",
      " [296 295 179 179]]\n",
      "Alındı.\n",
      "Face not detected\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[318 299 178 178]]\n",
      "Alındı.\n",
      "Face not detected\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[280 286 187 187]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[281 257 200 200]]\n",
      "Alındı.\n",
      "else\n",
      "5\n",
      ".\n",
      "Prediction: neutral\n",
      "[[277 261 200 200]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[281 260 202 202]]\n",
      "Alındı.\n",
      "else\n",
      "5\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1.5\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "#set the rectangle background to white\n",
    "rectangle_bgr = (255, 255, 255)\n",
    "# make a black image\n",
    "img = np.zeros((500, 500))\n",
    "#set some text\n",
    "text = \"VİDGA Projesi\"\n",
    "#get the width and height of the text box\n",
    "(text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0]\n",
    "#set the text start position\n",
    "text_offset_x = 10\n",
    "text_offset_y = img.shape[0] - 25\n",
    "#make the coords of the box with a small padding of two pixels\n",
    "box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height -2))\n",
    "cv2.rectangle(img, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "cv2.putText(img, text, (text_offset_x, text_offset_y), font, fontScale=font_scale, color= (0,0,0), thickness=1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "#cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "#cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #print(frame)\n",
    "    #frame = imutils.resize(frame, width=320, height=480)\n",
    "    #eye_Cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    #faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'frontal-face-data.xml')\n",
    "    #print(\"ret is \" + str(ret))\n",
    "    #cv2.imshow('VIDGA Emotion Recognition', frame) \n",
    "    #while True:\n",
    "        #cv2.imshow('VIDGA Emotion Recognition', frame)\n",
    "    if ret == True:\n",
    "        #cv2.imshow('VIDGA Emotion Recognition', frame)\n",
    "    \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #print(faceCascade.empty())\n",
    "        faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "        print(faces)\n",
    "        for x,y,w,h in faces:\n",
    "            print(\"Alındı.\")\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            facess = faceCascade.detectMultiScale(roi_gray)\n",
    "            if len(facess) == 0:\n",
    "                print(\"Face not detected\")\n",
    "            else: \n",
    "                print(\"else\")\n",
    "                for(ex,ey,ew,eh) in facess:\n",
    "                    face_roi = roi_color[ey: ey+eh, ex: ex+ew] ##cropping the face\n",
    "                \n",
    "    graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    final_image = cv2.resize(graytemp, (48,48))\n",
    "    final_image = np.expand_dims(final_image, axis =0) #add third dimension\n",
    "    final_image = np.expand_dims(final_image, axis =0) #add fourth dimension\n",
    "    final_image = final_image/255.0 # normalization\n",
    "    dataa = torch.from_numpy(final_image)\n",
    "    dataa = dataa.type(torch.FloatTensor)\n",
    "    dataa = dataa.to(device)\n",
    "    outputs = net(dataa)\n",
    "    Pred = F.softmax(outputs, dim=1)\n",
    "    Predictions = torch.argmax(Pred).item()\n",
    "    print(Predictions)\n",
    "    \n",
    "    #sound Recording and result\n",
    "    recordSound()\n",
    "    prediction = deeprec.predict('output.wav')\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    \n",
    "    ############ Here comes SQL for Face Emotion ############\n",
    "    import pyodbc \n",
    "\n",
    "    server = 'DESKTOP-T7OFQV6\\SQLEXPRESS1'\n",
    "    database = 'VidgaEmotionRecognition'\n",
    "\n",
    "    #defining connection string\n",
    "    cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + server + ';DATABASE=' + database +';Trusted_Connection=yes;')\n",
    "\n",
    "    #creating the connection cursor\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    #defining insert query\n",
    "    insert_query='''INSERT INTO FaceEmotion (emotion, dateOfNow)\n",
    "                    VALUES (?, ?);'''\n",
    "\n",
    "    #defining insertion value\n",
    "    values = [Predictions, datetime.now()]\n",
    "\n",
    "    #insert the data\n",
    "    cursor.execute(insert_query, values)\n",
    "\n",
    "    #commit the insertion\n",
    "    cnxn.commit()\n",
    "\n",
    "    #grab the database table values\n",
    "    cursor.execute('SELECT * FROM FaceEmotion')\n",
    "\n",
    "    #printing the results\n",
    "    \"\"\"for values in cursor:\n",
    "        print(values)\"\"\"\n",
    "    \n",
    "   ############ Here comes SQL for Audio Emotion ############ \n",
    "\n",
    "    #defining insert query for audio emotion\n",
    "    insert_query2='''INSERT INTO SpeechEmotion (emotion, dateOfNow)\n",
    "                    VALUES (?, ?);'''\n",
    "    \n",
    "    #defining insertion value\n",
    "    values2 = [prediction, datetime.now()]\n",
    "    \n",
    "    #insert the data\n",
    "    cursor.execute(insert_query2, values2)\n",
    "    \n",
    "    #commit the insertion\n",
    "    cnxn.commit()\n",
    "    \n",
    "    #grab the database table values\n",
    "    cursor.execute('SELECT * FROM FaceEmotion')\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    font_scale = 1.5\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    \n",
    "    if ((Predictions)==0):\n",
    "        status = \"Angry\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==1):\n",
    "        status = \"Disgust\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==2):\n",
    "        status = \"Fear\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==3):\n",
    "        status = \"Happy\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==4):\n",
    "        status = \"Sad\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==5):\n",
    "        status = \"Surprised\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==6):\n",
    "        status = \"Neutral\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "    \n",
    "    \n",
    "    if ret == True:\n",
    "        cv2.imshow('VIDGA Emotion Recognition', frame) \n",
    "            \n",
    "            \n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
