{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import pdb\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "import pyodbc \n",
    "from deep_emotion_recognition import DeepEmotionRecognizer\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TESS&RAVDESS] There are 809 training audio files for category:angry\n",
      "[TESS&RAVDESS] There are 148 testing audio files for category:angry\n",
      "[TESS&RAVDESS] There are 813 training audio files for category:sad\n",
      "[TESS&RAVDESS] There are 147 testing audio files for category:sad\n",
      "[TESS&RAVDESS] There are 586 training audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 94 testing audio files for category:neutral\n",
      "[TESS&RAVDESS] There are 514 training audio files for category:ps\n",
      "[TESS&RAVDESS] There are 78 testing audio files for category:ps\n",
      "[TESS&RAVDESS] There are 806 training audio files for category:happy\n",
      "[TESS&RAVDESS] There are 148 testing audio files for category:happy\n",
      "[+] Writed TESS & RAVDESS DB CSV File\n",
      "[EMO-DB] Total files to write: 339\n",
      "[EMO-DB] Training samples: 271\n",
      "[EMO-DB] Testing samples: 67\n",
      "[+] Writed EMO-DB CSV File\n",
      "[Custom Dataset] There are 49 training audio files for category:neutral\n",
      "[Custom Dataset] There are 33 testing audio files for category:neutral\n",
      "[Custom Dataset] There are 33 training audio files for category:ps\n",
      "[Custom Dataset] There are 33 testing audio files for category:ps\n",
      "[Custom Dataset] There are 48 training audio files for category:happy\n",
      "[Custom Dataset] There are 23 testing audio files for category:happy\n",
      "[+] Writed Custom DB CSV File\n",
      "[+] Data loaded\n",
      "[+] Model created\n",
      "[*] Model weights loaded\n",
      "0.5230769230769231\n"
     ]
    }
   ],
   "source": [
    "#Loading visual emotion recognition model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Deep_Emotion()\n",
    "net.load_state_dict(torch.load('DeepEmotion_trained_by_VIDGA.pt'))\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "#defining sound recording function\n",
    "#Loading audio emotion recognition model\n",
    "\n",
    "def recordSound():\n",
    "    print(\".\")\n",
    "    samplerate = 44100  # Hertz\n",
    "    duration = 1  # seconds\n",
    "    filename = 'output.wav'\n",
    "\n",
    "    mydata = sd.rec(int(samplerate * duration), samplerate=samplerate,\n",
    "                    channels=1, blocking=True)\n",
    "    sf.write(filename, mydata, samplerate)\n",
    "\n",
    "# initialize instance\n",
    "# inherited from emotion_recognition.EmotionRecognizer\n",
    "# default parameters (LSTM: 128x2, Dense:128x2)\n",
    "deeprec = DeepEmotionRecognizer(emotions=['angry', 'sad', 'neutral', 'ps', 'happy'], n_rnn_layers=2, n_dense_layers=2, rnn_units=128, dense_units=128)\n",
    "# train the model\n",
    "deeprec.train()\n",
    "# get the accuracy\n",
    "print(deeprec.test_score())\n",
    "# predict angry audio sample\n",
    "\n",
    "\n",
    "##################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[243 175 164 164]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: sad\n",
      "[[251 175 165 165]]\n",
      "Alındı.\n",
      "else\n",
      "0\n",
      ".\n",
      "Prediction: neutral\n",
      "[[252 176 165 165]]\n",
      "Alındı.\n",
      "else\n",
      "0\n",
      ".\n",
      "Prediction: neutral\n",
      "[[248 171 168 168]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[206  72 243 243]\n",
      " [245 170 170 170]]\n",
      "Alındı.\n",
      "else\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[246 168 172 172]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[245 168 173 173]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[244 169 175 175]]\n",
      "Alındı.\n",
      "else\n",
      "0\n",
      ".\n",
      "Prediction: neutral\n",
      "[[244 182 164 164]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[247 179 168 168]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: sad\n",
      "[[248 177 168 168]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[250 177 171 171]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[238 167 180 180]]\n",
      "Alındı.\n",
      "else\n",
      "4\n",
      ".\n",
      "Prediction: neutral\n",
      "[[243 177 168 168]]\n",
      "Alındı.\n",
      "else\n",
      "3\n",
      ".\n",
      "Prediction: neutral\n",
      "[[245 176 168 168]]\n",
      "Alındı.\n",
      "else\n",
      "3\n",
      ".\n",
      "Prediction: neutral\n",
      "[[246 180 170 170]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[252 187 172 172]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[248 180 170 170]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[249 179 172 172]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[249 181 172 172]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[246 173 176 176]]\n",
      "Alındı.\n",
      "else\n",
      "3\n",
      ".\n",
      "Prediction: happy\n",
      "[[253 178 172 172]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: angry\n",
      "[[250 185 170 170]]\n",
      "Alındı.\n",
      "else\n",
      "0\n",
      ".\n",
      "Prediction: neutral\n",
      "[[250 181 173 173]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[249 183 167 167]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[247 180 172 172]]\n",
      "Alındı.\n",
      "else\n",
      "0\n",
      ".\n",
      "Prediction: neutral\n",
      "[[244 181 173 173]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[246 182 167 167]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[248 179 168 168]]\n",
      "Alındı.\n",
      "else\n",
      "2\n",
      ".\n",
      "Prediction: neutral\n",
      "[[249 179 175 175]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n",
      "[[248 181 175 175]]\n",
      "Alındı.\n",
      "else\n",
      "6\n",
      ".\n",
      "Prediction: neutral\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1.5\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "#set the rectangle background to white\n",
    "rectangle_bgr = (255, 255, 255)\n",
    "# make a black image\n",
    "img = np.zeros((500, 500))\n",
    "#set some text\n",
    "text = \"VİDGA Projesi\"\n",
    "#get the width and height of the text box\n",
    "(text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0]\n",
    "#set the text start position\n",
    "text_offset_x = 10\n",
    "text_offset_y = img.shape[0] - 25\n",
    "#make the coords of the box with a small padding of two pixels\n",
    "box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height -2))\n",
    "cv2.rectangle(img, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "cv2.putText(img, text, (text_offset_x, text_offset_y), font, fontScale=font_scale, color= (0,0,0), thickness=1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "#cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "#cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #print(frame)\n",
    "    #frame = imutils.resize(frame, width=320, height=480)\n",
    "    #eye_Cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    #faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'frontal-face-data.xml')\n",
    "    #print(\"ret is \" + str(ret))\n",
    "    #cv2.imshow('VIDGA Emotion Recognition', frame) \n",
    "    #while True:\n",
    "        #cv2.imshow('VIDGA Emotion Recognition', frame)\n",
    "    if ret == True:\n",
    "        #cv2.imshow('VIDGA Emotion Recognition', frame)\n",
    "    \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #print(faceCascade.empty())\n",
    "        faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "        print(faces)\n",
    "        for x,y,w,h in faces:\n",
    "            print(\"Alındı.\")\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            facess = faceCascade.detectMultiScale(roi_gray)\n",
    "            if len(facess) == 0:\n",
    "                print(\"Face not detected\")\n",
    "            else: \n",
    "                print(\"else\")\n",
    "                for(ex,ey,ew,eh) in facess:\n",
    "                    face_roi = roi_color[ey: ey+eh, ex: ex+ew] ##cropping the face\n",
    "                \n",
    "    graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    final_image = cv2.resize(graytemp, (48,48))\n",
    "    final_image = np.expand_dims(final_image, axis =0) #add third dimension\n",
    "    final_image = np.expand_dims(final_image, axis =0) #add fourth dimension\n",
    "    final_image = final_image/255.0 # normalization\n",
    "    dataa = torch.from_numpy(final_image)\n",
    "    dataa = dataa.type(torch.FloatTensor)\n",
    "    dataa = dataa.to(device)\n",
    "    outputs = net(dataa)\n",
    "    Pred = F.softmax(outputs, dim=1)\n",
    "    Predictions = torch.argmax(Pred).item()\n",
    "    print(Predictions)\n",
    "    \n",
    "    #####updated part#####\n",
    "    if ((Predictions)==0):\n",
    "        newPredictions = \"angry\"\n",
    "        \n",
    "    elif ((Predictions)==1):\n",
    "        newPredictions = \"disgust\"\n",
    "    \n",
    "    elif ((Predictions)==2):\n",
    "        newPredictions = \"fear\"\n",
    "    \n",
    "    elif ((Predictions)==3):\n",
    "        newPredictions = \"happy\"\n",
    "    \n",
    "    elif ((Predictions)==4):\n",
    "        newPredictions = \"sad\"\n",
    "        \n",
    "    elif ((Predictions)==5):\n",
    "        newPredictions = \"surprised\"\n",
    "        \n",
    "    elif ((Predictions)==6):\n",
    "        newPredictions = \"neutral\"\n",
    "    \n",
    "    \n",
    "    #sound Recording and result\n",
    "    recordSound()\n",
    "    prediction = deeprec.predict('output.wav')\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    \n",
    "    ############ Here comes SQL for Face Emotion ############\n",
    "    import pyodbc \n",
    "\n",
    "    server = 'DESKTOP-T7OFQV6\\SQLEXPRESS1'\n",
    "    database = 'VidgaEmotionRecognition'\n",
    "\n",
    "    #defining connection string\n",
    "    cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + server + ';DATABASE=' + database +';Trusted_Connection=yes;')\n",
    "\n",
    "    #creating the connection cursor\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    #defining insert query\n",
    "    insert_query='''INSERT INTO FaceEmotion (emotion, dateOfNow)\n",
    "                    VALUES (?, ?);'''\n",
    "\n",
    "    #defining insertion value\n",
    "    values = [newPredictions, datetime.now()]\n",
    "\n",
    "    #insert the data\n",
    "    cursor.execute(insert_query, values)\n",
    "\n",
    "    #commit the insertion\n",
    "    cnxn.commit()\n",
    "\n",
    "    #grab the database table values\n",
    "    cursor.execute('SELECT * FROM FaceEmotion')\n",
    "\n",
    "    #printing the results\n",
    "    \"\"\"for values in cursor:\n",
    "        print(values)\"\"\"\n",
    "    \n",
    "   ############ Here comes SQL for Audio Emotion ############ \n",
    "\n",
    "    #defining insert query for audio emotion\n",
    "    insert_query2='''INSERT INTO SpeechEmotion (emotion, dateOfNow)\n",
    "                    VALUES (?, ?);'''\n",
    "    \n",
    "    #defining insertion value\n",
    "    values2 = [prediction, datetime.now()]\n",
    "    \n",
    "    #insert the data\n",
    "    cursor.execute(insert_query2, values2)\n",
    "    \n",
    "    #commit the insertion\n",
    "    cnxn.commit()\n",
    "    \n",
    "    #grab the database table values\n",
    "    cursor.execute('SELECT * FROM FaceEmotion')\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    font_scale = 1.5\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    \n",
    "    if ((Predictions)==0):\n",
    "        status = \"Angry\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==1):\n",
    "        status = \"Disgust\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==2):\n",
    "        status = \"Fear\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==3):\n",
    "        status = \"Happy\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==4):\n",
    "        status = \"Sad\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==5):\n",
    "        status = \"Surprised\"\n",
    "        \n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "        \n",
    "    elif ((Predictions)==6):\n",
    "        status = \"Neutral\"\n",
    "\n",
    "        x1,y1,w1,h1, = 0,0,175,5\n",
    "        # Draw black background rectangle\n",
    "        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "\n",
    "        cv2.putText(frame, status, (100,150), font, 3,(0,0,255), 2, cv2.LINE_4)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
    "    \n",
    "    \n",
    "    if ret == True:\n",
    "        cv2.imshow('VIDGA Emotion Recognition', frame) \n",
    "            \n",
    "            \n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
